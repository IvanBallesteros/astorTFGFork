# LLM Integration in Astor: Technical Report

This document describes the implementation of a new feature based on Large Language Models (LLMs) to extend the capabilities of the [Astor](https://github.com/SpoonLabs/astor) automatic program repair framework. The goal is to enable LLMs to participate in the **Patch Generation** phase. This implementation is designed to be modular, extensible, and easy to deploy using local models via [Ollama](https://ollama.com/).

---

## Architecture Overview

The architecture follows a modular structure that allows Patch Generation's LLM to be changed by any other not doing many changes and reusing existing code. In this version, the LLM has been integrated into the **Patch Generation** phase, as a candidate patch generator, but classes created could be reused to implement LLM on Validation and Fould Localization phases.

```
            +-------------------+
            |    User Client    |
            +---------+---------+
                      |
                      |   command line    
                      |
                      v
 +-----------------------------------------+
 |                  ASTOR                  |
 +-----------------------------------------+
 |                                         |
 |    +-------------------------------+    |
 |    |       Fault Localization      |    |    API (HTTP protocol)
 |    +-------------------------------+    |    Prompt              
 |    +-------------------------------+    |                      +---------------------+
 |    |        Patch Generation       |<------------------------->|     LLM Service     |
 |    +-------------------------------+    |                      +---------------------+
 |    +-------------------------------+    |    JSON response     |                     |
 |    |          Validation           |    |                      |  - Ollama           |
 |    +-------------------------------+    |                      |  - Local execution  |
 |                                         |                      |                     |
 +-----------------------------------------+                      +---------------------+

```

---

## Folder and File Structure

```
astor/
│
├── src/
│   ├── fr/inria/astor/...
│   └── fr/inria/astor/core/ingredientbased/     <- Location
│       ├── LLMIngredientEngine.java             <- Engine class for program repair with LLMs
│       ├── LLMPromptTemplate.java               <- Prompt threatment class
│       └── LLMService.java                      <- Service for comunication Astor - Ollama and parsiong JSON response

```

---

## Language Model Used

The solution uses an LLM running locally via Ollama. By default, the model is `codellama:7b`, but this can be customized through configuration or CLI arguments "-parameters". Every API protocol LLM available on Ollama can be used as iplementation allows. 

**Advantages of Ollama:**

* Does not require internet connection.
* Fully local execution.
* Simple RESTful HTTP interface.
* Open-source.

---

## Communication with Ollama

Communication with Ollama is done via HTTP `POST` requests to the `/api/generate` endpoint. Prompts are generated by editing parameters on call execution. 

Parameters list:

* llmService: LLM selecction
* llmmodel: Model from LLM selected previously
* maxSuggestionsPerPoint: Number of different solutions LLM has to provide. The more suggestions, more provability to fix code but it will take more time to execute
* llmprompttemplate: Template for prompt confection

### Parameters example:

```Parameters

llmService = "ollama"
llmmodel = "codellama:13b"
maxSuggestionsPerPoint = 4
llmprompttemplate = "GUIDED_SOLUTIONS"

```

The Java service (`LLMService`) handles both request sending and response parsing to generate valid patched code.

---

## How to Enable LLM in Astor

1. Make sure you have [Ollama installed and running](https://ollama.com/download).
2. Load the required model you want to use:

   ```bash
   ollama pull "model name"
   ```
3. Run the model downloaded:

   ```bash
   ollama run "model name"
   ```
4. Run Astor using the following flags:

* Add "-parameters" varaibles as mentioned on `Comunication with Ollama`.
* Add flags:
    - "-mode" = "custom"
    - "-customengine" = "LLMIngredientEngine.class.getName()"

---

## Prompting Strategy

Prompts are designed to be simple yet specific:

```java
    "GUIDED_SOLUTIONS",
    "You are an automated code-fixing agent. Your job is to replace a buggy Java line with a modification of it.\n" +
    "Buggy Java line code:\n{buggycode}\n\n" +
    "Failing test case:\n{testcode}\n\n" +
    "Generate {nsolutions} single-line Java code replacements that may fix the bug.\n" +
    "Please, apply just one small edit on the line provided.\n" +
    "DO NOT include any explanations or extra text.\n" +
    "ONLY output code, strictly formatted as:\n" +
    "SOLUTION 1:\n<code>\n" +
    "SOLUTION 2:\n<code>\n" +
    "...SOLUTION {nsolutions}:\n<code>\n" 
```

Information on varaibles mean the following:

* buggycode: Corresponding to the first line of the modification point on that instance.
* testcode: Name of the test failing.
* nsolutions: Number of alternative solutions LLM has to provide. Is the same value as `maxsuggestionsperpoint` variable.

This helps the model focus on producing syntactically valid and semantically coherent patches.
If you want to add more templates, do it on class `LLMPromptTemplate`.

---

## Output and Metrics

Execution results are managed by `LLMService` class to parse and clean JSON response provided by the LLM. Then, they're converted to an array of candidate solutions to be replaced on code by the modification point using ReplaceOp.

Once a patch is generated, Astor continues with its workflow to Validation phase. If parameter "-stopfirst" is on `true`, as far as one patch runs all tests and is validated, framework stops searching more candiates.

---

## Testing 

To validate the system's behavior after integrating the LLM, we used the QuixBugs benchmark suite. QuixBugs includes multiple buggy Java programs along with their corresponding fixed versions and a predefined set of test cases. This allows for automated evaluation of whether a patch generated by Astor+LLM correctly fixes the bug without introducing regressions.

All the testing logic is centralized in the following class:

```bash
\src\test\java\fr\inria\astor\test\repair\QuixBugsRepairTestLLM.java
```

This class runs automated tests on various buggy programs from QuixBugs by applying the repair process with LLM and verifying whether the generated patches allow the tests to pass. This ensures that the integration behaves as expected and that the results are both measurable and reproducible.

At the end of execution, it's shown a summary with execution time of each test, number of tests passed and failed and names of them.

---

## Limitations

* Patch quality depends heavily on the selected model.
* Semantic inconsistencies may occur if the prompt is poorly designed.
* LLM-based patching is currently single-threaded (no parallel generation yet).

---

## Future Work

* Extend LLM usage to *Fault Localization* and *Validation* phases.
* Add automatic patch scoring based on heuristics or coverage.
* Implement dynamic prompt tuning based on validation feedback.

---
